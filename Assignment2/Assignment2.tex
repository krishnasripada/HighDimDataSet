\documentclass[10pt]{article}
\usepackage{amsmath}
\usepackage{mathtools}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\usepackage[hidelinks]{hyperref}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{caption}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\graphicspath{{.}}
\usepackage{listings}
\usepackage{verbatim}
\usepackage{floatrow}
\usepackage{bigints}
\lstset{
language=[LaTeX]TeX,
backgroundcolor=\color{gray!25},
basicstyle=\ttfamily,
columns=flexible,
breaklines=true
}
\captionsetup{labelsep=space,justification=centering,singlelinecheck=off}
\reversemarginpar
\usepackage[paper=a4paper,
            %includefoot, % Uncomment to put page number above margin
            marginparwidth=10mm,      % Length of section titles
            marginparsep=0.8mm,       % Space between titles and text
            margin=11mm,              % 25mm margins
            includemp]{geometry}

\begin{document}
\section*{}
\begin{flushleft}
Name: Krishna Chaitanya Sripada\\
\end{flushleft}
\section*{Ans 1}
\begin{flushleft}
To prove $O(n)$ is a group when equipped with the matrix multiplication, we need to prove that the 4 points mentioned in Definition 1 hold good.\\
\vspace{0.5em}
\textbf{Point 1:} We need to prove that on multiplying two orthogonal matrices in $O(n)$, we get back an orthogonal matrix thus meaning the resultant matrix is also in $O(n)$.\\
Let us consider two orthogonal matrices G and H, then,\\
\vspace{0.5em}
$(G G^{T}) (H H^{T}) = (G H) (G^{T} H ^{T}) = (H G) (H G)^{T} = I$. This means that the resultant matrix is in the group $O(n)$.\\
\vspace{0.5em}
\textbf{Proof by example:} $A_{n \times n}$ is an orthogonal matrix if, $A A^{T} = A^{T} A = I$. Consider the multiplication of following orthogonal matrices,\\
$$
\begin{bmatrix} 
-1 & 0 & 0 \\
0 & -1 & 0 \\
0 & 0 & -1 
\end{bmatrix}
\begin{bmatrix} 
0 & -1 & 0 \\
1 & 0 & 0 \\
0 & 0 & -1 
\end{bmatrix}
= \begin{bmatrix} 
0 & 1 & 0 \\
-1 & 0 & 0 \\
0 & 0 & 1 
\end{bmatrix}
 = A
$$
\\
The transpose of this matrix is ,\\
$$
A^{T} =
\begin{bmatrix} 
0 & -1 & 0 \\
1 & 0 & 0 \\
0 & 0 & 1 
\end{bmatrix}
$$
\\
Therefore, 
$$
A A^{T} =
\begin{bmatrix} 
0 & 1 & 0 \\
-1 & 0 & 0 \\
0 & 0 & 1 
\end{bmatrix}
\begin{bmatrix} 
0 & -1 & 0 \\
1 & 0 & 0 \\
0 & 0 & 1 
\end{bmatrix}
= 
\begin{bmatrix} 
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 
\end{bmatrix}
 = I
$$
\\
Also,
$$
A^{T} A=
\begin{bmatrix} 
0 & -1 & 0 \\
1 & 0 & 0 \\
0 & 0 & 1 
\end{bmatrix}
\begin{bmatrix} 
0 & 1 & 0 \\
-1 & 0 & 0 \\
0 & 0 & 1 
\end{bmatrix}
= 
\begin{bmatrix} 
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 
\end{bmatrix}
 = I
$$
\\
The resultant matrix also belongs to group $O(n)$.\\
\vspace{0.5em}
\textbf{Point 2:}  We need to prove that matrix multiplication of 3 matrices that belong to $O(n)$ is associative.\\
Let us consider three orthogonal matrices F, G and H, then,\\
\vspace{0.5em}
$\sum_{p} \sum_{q} F_{ip} G_{pq} H_{qj} = \sum_{p} F_{ip} (\sum_{q} G_{pq} H_{qj}) = \sum_{p} F_{ip} (GH)_{pj} = F(GH)$\\
\vspace{0.5em}
$\sum_{p} \sum_{q} F_{ip} G_{pq} H_{qj} = \sum_{q} (\sum_{p} F_{ip} G_{pq}) H_{qj} = \sum_{q} (FG)_{iq} H_{qj} = (FG)H$\\
\vspace{0.5em}
\textbf{Point 3:} 
Let the element that belongs to the group $O(n)$ be an identity matrix (as Identity matrix is also orthogonal), then for another orthogonal matrix G $\in O(n)$, we get,\\
\vspace{0.5em}
$G (I) = I (G) = G$. This is because matrix multiplication of an identity matrix and any other matrix is commutative in nature and returns the original matrix. \\
\vspace{0.5em}
\textbf{Point 4:}
We need to prove that the multiplication of an inverse and the matrix itself is an identity matrix which also belongs to the group $O(n)$.\\
Since, inverse of an orthogonal matrix is also orthogonal, it follows the Point 1 stated above where we check for the multiplication of two orthogonal matrices. \\
In this case, we get,\\
$G (G^{-1}) = I$ and $G^{-1} (G) = I$. Thus we can prove that the identity matrix also belongs to $O(n)$, we can say an inverse exists in $O(n)$.
\end{flushleft}
\section*{Ans 2}
\begin{flushleft}
Since $U \in O(n)$, this means that U is in the group of orthogonal matrices, so for any matrix M that belongs to this group, we know that, \\
\vspace{0.5em}
$M^{T} = M^{-1}$ (The transpose of M is the inverse of M). \\
\vspace{0.5em}
Also, $M M^{-1} = I$\\
\vspace{0.5em}
Therefore, \\
\vspace{0.5em}
$M M^{T} = I$ \\
\vspace{0.5em}
Now taking a determinant on both sides,
\vspace{0.5em}
$det ( M M^{T}) = det I$\\
\vspace{0.5em}
We know that $det M * det M^{T} = 1$ (since $det I = 1$).\\
\vspace{0.5em}
Now $det M * det M = 1$ (since $det M^{T} = det M$)\\
\vspace{0.5em}
Meaning $(det M)^{2} = 1$\\
\vspace{0.5em}
Therefore, det(M) = $\pm$ 1.
\end{flushleft}
\section*{Ans 3}
\begin{flushleft}
To prove $SO(n)$ is a subgroup of $O(n)$, we need to prove that the 2 points mentioned in Definition 2 hold good.\\
\vspace{0.5em}
\textbf{Point 1:} To prove this, let us consider two orthogonal matrices G, H $\in$ $SO(n)$, this means that the product of two orthogonal matrices should belong to the $SO(n)$.\\
\vspace{0.5em}
\textbf{Proof by example:} Consider the multiplication of the following orthogonal matrices that below to $SO(n)$. \\
$$
\begin{bmatrix} 
1 & 0 & 0 \\
0 & -1 & 0 \\
0 & 0 & -1 
\end{bmatrix} 
\begin{bmatrix} 
-1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & -1 
\end{bmatrix} 
= 
\begin{bmatrix} 
-1 & 0 & 0 \\
0 & -1 & 0 \\
0 & 0 & 1 
\end{bmatrix} 
$$
\\
The determinant of the resultant matrix  = 1 which belongs to $SO(n)$ which is an special orthogonal matrix. Thus the resultant matrix also belongs to $SO(n)$.\\
\vspace{0.5em}
\textbf{Point 2: } We need to prove that the inverse of a matrix and the multiplication of the same matrix is an identity matrix. \\
\vspace{0.5em}
Since, inverse of an orthogonal matrix is also orthogonal, it follows the Point 1 stated above where we check for the multiplication of two orthogonal matrices. \\
In this case, we get,\\
$G (G^{-1}) = I$ and $G^{-1} (G) = I$. Thus we can prove that the identity matrix also belongs to $SO(n)$, we can say an inverse exists in $SO(n)$.
\end{flushleft}
\section*{Ans 4}
\begin{flushleft}
Given that $$
e_{1} = 
\begin{bmatrix} 
1 \\
0 \\
\vdots \\
0 
\end{bmatrix} 
$$
and $\mathcal{G} = \{U \in SO(n), U e_{1} = e_{1}\}$\\
\vspace{0.5em}
Let two elements $U_{1}, U_{2} \in \mathcal{G}$, then $U_{1} e_{1} = e_{1}$ and $U_{2} e_{1} = e_{1}$.\\
\vspace{0.5em}
(a) Property 1 Proof: Since every element in $\mathcal{G} \in SO(n)$, we have, \\
\vspace{0.5em}
$U_{1} U_{2} (U_{1} U_{2})^{T} = U_{1} U_{2} U_{2}^{T} U_{1}^{T} = I*I = I$ and \\
\vspace{0.5em}
$(U_{1} U_{2}) e_{1} = U_{1} U_{2} e_{1} = U_{1} e_{1} = e_{1}$. \\
\vspace{0.5em}
Therefore $\mathcal{G}$ is closed under the operator *.\\
\vspace{0.5em}
(b) Property 2 Proof: Since $U \in SO(n)$, we have ,\\
\vspace{0.5em}
$U^{-1} = U^{T}$ and $U*U^{T}=I$. Therefore, $U*U^{-1} = U^{-1}*U = I$. Therefore $U^{-1} \in SO(n)$.\\
\vspace{0.5em}
Hence $\mathcal{G}$ is a subgroup of SO(n).
\end{flushleft}
\section*{Ans 5}
\begin{flushleft}
Given that W is $n-1 \times n-1$ matrix that belongs to $SO(n-1)$. That means, det(W) =1 and $WW^{T}=I$. \\
\vspace{0.5em}
Then $$
U U^{T}=  
\begin{bmatrix}
    1 & 0 \\
    0 & W
\end{bmatrix} 
\begin{bmatrix}
    1 & 0 \\
    0 & W^{T}
\end{bmatrix}
= \begin{bmatrix}
    1 & 0 \\
    0 & WW^{T}
\end{bmatrix}
$$
$WW^{T} = I$ only if W $\in SO(n-1)$ and since it is given, $UU^{T}= I$. Property 1 holds true.\\
\vspace{0.5em}
And, \\
\vspace{0.5em}
$$
U e_{1} = 
\begin{bmatrix}
    1 & 0 \\
    0 & W
\end{bmatrix} 
\begin{bmatrix} 
1 \\
0 \\
\vdots \\
0 
\end{bmatrix} 
$$
Since W is $n-1 \times n-1$ matrix, the result is $e_{1}$. Thus, property 2 holds true.\\
\vspace{0.5em}
We can say every element U in $\mathcal{G}$ is of the form
$$
\begin{bmatrix}
    1 & 0 \\
    0 & W
\end{bmatrix} 
$$
\end{flushleft}
\section*{Ans 6}
\begin{flushleft}
(a) Since $\mathcal{G} \longrightarrow SO(n-1)$ and $U \longmapsto \Psi(U) = W$ and if $U_{1}U_{2} \in \mathcal{G}$, we have,\\
\vspace{0.5em}
$$
\Psi(U_{1} U_{2})= 
\begin{bmatrix}
    1 & 0 \\
    0 & W_{1}
\end{bmatrix}
\begin{bmatrix}
    1 & 0 \\
    0 & W_{2}
\end{bmatrix}
= 
\begin{bmatrix}
    1 & 0 \\
    0 & W_{1}W_{2}
\end{bmatrix}
$$
$$
\Psi(U_{1}) = 
\begin{bmatrix}
    1 & 0 \\
    0 & W_{1}
\end{bmatrix}
$$
$$
\Psi(U_{2}) = 
\begin{bmatrix}
    1 & 0 \\
    0 & W_{2}
\end{bmatrix}
$$
$$
\Psi(U_{1}) \Psi(U_{2}) =
\begin{bmatrix}
    1 & 0 \\
    0 & W_{1}
\end{bmatrix}
\begin{bmatrix}
    1 & 0 \\
    0 & W_{2}
\end{bmatrix}
= 
\begin{bmatrix}
    1 & 0 \\
    0 & W_{1}W_{2}
\end{bmatrix}
$$
(b) To prove $\Psi$ is bijective, it needs to be injective and surjective.\\
\vspace{0.5em}
(i) Since $\Psi(U) = \Psi(V)$ and U, V $\in \mathcal{G}$, we must prove that U = V. Since for any output  of the $\Psi$ function gives the same result which is `W'. That means that the inputs are also equal for those outputs i.e., U = V.\\
\vspace{0.5em}
(ii) Since $\Psi(U) = W$ and $\Psi(U) = Q$, we get, W = Q and since Q $\in$ SO(n-1), W $\in$ SO(n-1), we get that,\\ 
$$
U =
\begin{bmatrix}
    1 & 0 \\
    0 & W
\end{bmatrix}
$$
which states U $\in \mathcal{G}$. Hence proved it is surjective as well. Thus function $\Psi$ is bijective.
\end{flushleft}
\section*{Ans 7}
\begin{flushleft}
For any $x \in S^{n-1}$, there exists a $U \in SO(n)$. For the condition $U e_{1} = x$ to be satisfied, U can be constructed in such a way that the first column of U is similar to x and all other columns in U are both orthogonal to each other and the first column. Only then , the condition can be satisfied. Thus this makes U not to be unique as you can have multiple columns in U where they are orthogonal to each other and the first column since $U \in SO(n)$.
\end{flushleft}
\section*{Ans 8}
\begin{flushleft}
Given U and V are two n $\times$ n matrices in SO(n) such that $Ue_{1} = Ve_{1} = x$. For the condition
$$
U = V
\begin{bmatrix}
    1 & 0 \\
    0 & W
\end{bmatrix}
$$
to be satisfied, to get a n $\times$ n matrix after multiplying an n$\times$n matrix with another n$\times$n matrix that contains W, it is necessary that W must $\in$ SO(n-1) so that resulting U can be orthogonal and also matrix multiplication V and 
$$
\begin{bmatrix}
    1 & 0 \\
    0 & W
\end{bmatrix}
$$
will be possible.
\end{flushleft}
\section*{Ans 9}
\begin{flushleft}
\end{flushleft}
\section*{Ans 10}
\begin{flushleft}
Since $\Omega \in SO(n)$ and $U \in \frac{SO(n)}{\mathcal{G}}$, we have $\varphi(\Omega U) = \Omega U e_{1}$ and since $\varphi(U) = U e_{1}$, we finally get $\varphi(\Omega U) = \Omega \varphi(U)$. Also we can state that multiplication of $\Omega U$ which result in a mapping that respects the action of $SO(n)$ since $\Omega$ and $U$ already $\in SO(n)$. Hence proved.
\end{flushleft}
\section*{Ans 11}
\begin{flushleft}
(a) To prove that $\varphi$ is bijective, we need to prove that:\\
\vspace{0.5em}
\begin{enumerate}
\item for any U,V $\in \mathcal{G}$, $\varphi(U) = \varphi(V) \Longrightarrow U = V$. 
\item for any $x \in S^{n-1}$, there exists $U \in \frac{SO(n)}{\mathcal{G}}$, such that $\varphi(U) = x$.
\end{enumerate}
To prove that the map $\varphi$ is injective, we know that $\varphi(U) = U e_{1}$ and $\varphi(V) = V e_{1}$. Since $\varphi(U) = \varphi(V)$, we have $U e_{1} = V e_{1}$. Since U $\in \mathcal{G}$ and V $\in \mathcal{G}$, from this we get $U=V$. Hence map $\varphi$ is injective.\\
\vspace{0.5em}
To prove that the map $\varphi$ is surjective, since $\varphi(U) = U e_{1}$. We get that $U e_{1} = x$. This proves that $U \in SO(n)$. Since $\mathcal{G}$ is a subgroup of SO(n), we can state that $U$ also $\in \frac{SO(n)}{\mathcal{G}}$. Hence proved.\\
\vspace{0.5em}
Thus $\varphi$ is bijective.\\
\vspace{0.5em}
(b) The map $\varphi$ is continuous at a point `a' $\in S^{n-1}$ if,\\
\vspace{0.5em}
$\forall \epsilon \textgreater 0$, $\exists \delta \textgreater 0$, such that $\textbar x - a \textbar \textless \delta \Longrightarrow \textbar \varphi(x) - \varphi(a) \textbar \textless \epsilon$.\\
\vspace{0.5em}
Since $\varphi(U) = U e_{1}$, we need to prove that:\\
\vspace{0.5em}
$\forall \epsilon \textgreater 0$, $\exists \delta \textgreater 0$, such that $\textbar x - a \textbar \textless \delta \Longrightarrow \textbar x e_{1} - a e_{1} \textbar \textless \epsilon$.
\end{flushleft}
\end{document}